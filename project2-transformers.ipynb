{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install emoji\n",
    "# !pip install pyspellchecker\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib tqdm nltk\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.synchronize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import random\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to correct spelling mistakes in a string\n",
    "def correct_spelling(text):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word != None:\n",
    "          corrected_words.append(corrected_word)\n",
    "        # print(corrected_word)\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"w/\": \"with\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"msg\": \"message\",\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"LOL\": \"laughing out loud\",\n",
    "    \"XD\" : \"laugh\",\n",
    "    \"xd\" : \"laugh\"\n",
    "}\n",
    "\n",
    "# Define a function to correct abbreviations in a string\n",
    "def correct_abbreviations(text, abbreviations):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word in abbreviations:\n",
    "            corrected_word = abbreviations[word]\n",
    "            corrected_words.append(corrected_word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('TweetsCOV19_preprocessed.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10472 entries, 0 to 10471\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Sentiment     10472 non-null  object\n",
      " 1   TweetText     10462 non-null  object\n",
      " 2   tweet_length  10472 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 245.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10472 entries, 0 to 10471\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Sentiment     10472 non-null  object\n",
      " 1   TweetText     10462 non-null  object\n",
      " 2   tweet_length  10472 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 245.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# way too fucking slow\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "df_new = df[:]\n",
    "# df_new[\"tweet\"] = df_new[\"TweetText\"].progress_apply(preprocess_tweets)\n",
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataset(sentiment):\n",
    "    try:\n",
    "        y = np.fromstring(sentiment, dtype=int, sep=' ')[:2]\n",
    "        return y\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       x  Y_pos  Y_neg\n",
      "0      hey re go back listen rip hung cry jump proud ...      3     -4\n",
      "1      arrive rainstorm appear little slower thursday...      1     -1\n",
      "2      little munchkin little fan show stopper today ...      1     -3\n",
      "3      already cancel publicly time got uncouth boor ...      2     -4\n",
      "4      hardware store maybe nicole hardware appear so...      1     -1\n",
      "...                                                  ...    ...    ...\n",
      "10467  look julian one peopl share grievance onto mas...      1     -4\n",
      "10468  mi african rapper best rapper africa mean they...      1     -1\n",
      "10469  serv fellow man said definite man ist unwrap b...      1     -3\n",
      "10470  skee ball retail appear pinball barca tonight ...      3     -1\n",
      "10471        sorry brother happen youfeel respect : : to      3     -2\n",
      "\n",
      "[10472 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_intermediate = pd.DataFrame()\n",
    "df_intermediate['x'] = df_new['TweetText'].astype('<U1').to_numpy() # string\n",
    "df_intermediate['Y'] = df_new['Sentiment'].apply(make_dataset).to_numpy()\n",
    "\n",
    "# Drop non-values or non-NumPy arrays from Y column\n",
    "df_intermediate = df_intermediate.dropna()\n",
    "df_intermediate = df_intermediate[df_intermediate['Y'].apply(lambda x: isinstance(x, np.ndarray))]\n",
    "\n",
    "\n",
    "# Split Y column into Y_pos and Y_neg columns\n",
    "df_intermediate[['Y_pos', 'Y_neg']] = pd.DataFrame(df_intermediate['Y'].tolist())\n",
    "\n",
    "# Remove the original Y column\n",
    "df_intermediate.drop('Y', axis=1, inplace=True)\n",
    "df_intermediate = df_intermediate.dropna()\n",
    "\n",
    "print(df_intermediate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>Y_pos</th>\n",
       "      <th>Y_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hey re go back listen rip hung cry jump proud ...</td>\n",
       "      <td>3</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arrive rainstorm appear little slower thursday...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>little munchkin little fan show stopper today ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>already cancel publicly time got uncouth boor ...</td>\n",
       "      <td>2</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hardware store maybe nicole hardware appear so...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10467</th>\n",
       "      <td>look julian one peopl share grievance onto mas...</td>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10468</th>\n",
       "      <td>mi african rapper best rapper africa mean they...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10469</th>\n",
       "      <td>serv fellow man said definite man ist unwrap b...</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10470</th>\n",
       "      <td>skee ball retail appear pinball barca tonight ...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10471</th>\n",
       "      <td>sorry brother happen youfeel respect : : to</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10472 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       x  Y_pos  Y_neg\n",
       "0      hey re go back listen rip hung cry jump proud ...      3     -4\n",
       "1      arrive rainstorm appear little slower thursday...      1     -1\n",
       "2      little munchkin little fan show stopper today ...      1     -3\n",
       "3      already cancel publicly time got uncouth boor ...      2     -4\n",
       "4      hardware store maybe nicole hardware appear so...      1     -1\n",
       "...                                                  ...    ...    ...\n",
       "10467  look julian one peopl share grievance onto mas...      1     -4\n",
       "10468  mi african rapper best rapper africa mean they...      1     -1\n",
       "10469  serv fellow man said definite man ist unwrap b...      1     -3\n",
       "10470  skee ball retail appear pinball barca tonight ...      3     -1\n",
       "10471        sorry brother happen youfeel respect : : to      3     -2\n",
       "\n",
       "[10472 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x,y1,y2 = df_intermediate['x'], df_intermediate['Y_pos'], df_intermediate['Y_neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0]]),\n",
       " array([[0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_intermediate is your dataframe\n",
    "\n",
    "# Create one-hot encodings for y1\n",
    "y1_one_hot = pd.get_dummies(df_intermediate['Y_pos'], prefix='Y_pos', dtype=int).to_numpy()\n",
    "\n",
    "# Create one-hot encodings for y2\n",
    "y2_one_hot = pd.get_dummies(df_intermediate['Y_neg'], prefix='Y_neg', dtype=int).to_numpy()\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original dataframe\n",
    "# df_encoded = pd.concat([df_intermediate['x'], y1_one_hot, y2_one_hot], axis=1)\n",
    "# df_encoded\n",
    "y1_one_hot, y2_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0]],\n",
       "\n",
       "       [[1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1]],\n",
       "\n",
       "       [[1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0]],\n",
       "\n",
       "       [[0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1]],\n",
       "\n",
       "       [[0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Horizontally stack y1 and y2 into Y\n",
    "Y = np.concatenate([y1_one_hot, y2_one_hot], axis=1)\n",
    "\n",
    "Y = Y.reshape(-1, 2, 5)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel, AutoTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=y_train[:,0].shape[1])\n",
    "# model.softmax_f = nn.Softmax(dim=1)\n",
    "\n",
    "# two separate classifiers\n",
    "model.classifier1 = nn.Sequential(\n",
    "    nn.Linear(model.config.hidden_size, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 5),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# # Define the second classifier with softmax activation\n",
    "# model.classifier2 = nn.Sequential(\n",
    "#     nn.Linear(model.config.hidden_size, 16),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(16, 5),\n",
    "#     nn.Softmax(dim=1)\n",
    "# )\n",
    "\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Split dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert dataset to PyTorch tensors\n",
    "train_encodings = tokenizer(list(x_train), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(x_test), truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=158, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels1, labels2):\n",
    "        self.encodings = encodings\n",
    "        self.labels1 = labels1\n",
    "        self.labels2 = labels2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels1'] = torch.tensor(self.labels1[idx], dtype=torch.float)\n",
    "        item['labels2'] = torch.tensor(self.labels2[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = SentimentDataset(train_encodings, y_train[:,0], y_train[:,1])\n",
    "test_dataset = SentimentDataset(test_encodings, y_test[:,0], y_test[:,1])\n",
    "\n",
    "# Define dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=64)\n",
    "\n",
    "# Define loss function, optimizer, and learning rate scheduler\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)#, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 158])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155/1923553546.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataloader:\n",
    "    print(item['input_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del input_ids,attention_mask,labels1,labels2,logits1,logits2\n",
    "torch.cuda.empty_cache()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3006ff343049fe95b99c9c038db481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155/1923553546.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1dccda8b60447192b1a0fcd1e3e17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6d0ef8171249699fba57d406594436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6596d904d014b09818df0d704c68a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcd8219fd514f53965c00291ede3cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e6a5947e6140ecb0883a35e1b8baa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7a0d36867d40849937dec28f846c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55eaba7833684750892af6178f9c9875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5108e5d35b0b46d281febcc68c86bd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f771c4eb5944d96830c6377e2e66699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8201f1b7654fbd8cf4d7b8468b9134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3ff354abf04c9080517b1bea76f0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96349e9d048747e48938916ca2f76e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa79081e0a949dc850cc98b810c32bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff889ce9f86947f799897ac3049906fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6544fb7bcdba483184cce7fcc22454dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5d371db8de4808b78de49e90c351f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4b494019ef43cfabb24aefee61b43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4329fb8591413fbb475513c6286cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0134b41fc8ae4b1d83ff8e661c1d04db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56029fd7838e4e1db301259d2334bef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a8bb9e97f941f6a7aad2adfeb9c9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4289ecd657eb4759b49670e8c5a60e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80eeb67dda3b430288d5a589d2ce1609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3374914ad1da44ea84bd618101c1ce7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c10544114104d8bbabd89665291008c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962c84cb920a4ae78a67beb56ea525f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    total_train_loss_1 = 0\n",
    "    # total_train_loss_2 = 0\n",
    "    batch_id = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch_id += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        # input_ids = input_ids.float()\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        labels1 = batch['labels1'].to(device)\n",
    "        # labels2 = batch['labels2'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)#.float()\n",
    "        logits1 = model.classifier1(outputs.last_hidden_state[:, 0, :])\n",
    "        # logits1 = model.classifier1(outputs.last_hidden_state[:, 0, :])\n",
    "        # logits2 = model.classifier2(outputs.last_hidden_state[:, 0, :])\n",
    "              \n",
    "        loss1 = loss_function(logits1, labels1)\n",
    "        # loss2 = loss_function(logits2, labels2)\n",
    "        \n",
    "        total_train_loss_1 += loss1.item()\n",
    "        # total_train_loss_2 += loss2.item()\n",
    "        \n",
    "        loss1.backward()\n",
    "        # loss2.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        del input_ids,attention_mask,labels1, logits1#,labels2,logits1,logits2\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "        # if batch_id % 150 == 0:\n",
    "           \n",
    "            # Evaluation\n",
    "    model.eval()\n",
    "    total_eval_loss_1 = 0\n",
    "    # total_eval_loss_2 = 0\n",
    "    test_batch_num = 0\n",
    "    for batch in test_dataloader:\n",
    "        test_batch_num += 1\n",
    "        if test_batch_num > 25:\n",
    "            break\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels1 = batch['labels1'].to(device)\n",
    "        # labels2 = batch['labels2'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits1 = model.classifier1(outputs.last_hidden_state[:, 0, :])\n",
    "            # logits1 = model.classifier1(outputs.last_hidden_state[:, 0, :])\n",
    "            # logits2 = model.classifier2(outputs.last_hidden_state[:, 0, :])\n",
    "\n",
    "            loss1 = loss_function(logits1, labels1)\n",
    "            # loss2 = loss_function(logits2, labels2)\n",
    "\n",
    "        total_eval_loss_1 += loss1.item()\n",
    "        # total_eval_loss_2 += loss2.item()\n",
    "\n",
    "        del input_ids,attention_mask,labels1,logits1,#labels2,logits1,logits2\n",
    "\n",
    "    avg_train_loss_1 = total_train_loss_1 / test_batch_num\n",
    "    # avg_train_loss_2 = total_train_loss_2 / test_batch_num\n",
    "    avg_eval_loss_1 = total_eval_loss_1 / test_batch_num\n",
    "    # avg_eval_loss_2 = total_eval_loss_2 / test_batch_num\n",
    "\n",
    "    # print(f\"Average training loss (Y_pos): {avg_train_loss_1:.2f}\")\n",
    "    # print(f\"Average training loss (Y_neg): {avg_train_loss_2:.2f}\")\n",
    "    # print(f\"Average evaluation loss (Y_pos): {avg_eval_loss_1:.2f}\")\n",
    "    # print(f\"Average evaluation loss (Y_neg): {avg_eval_loss_2:.2f}\")c\n",
    "\n",
    "    logging.info(f\"BATCH stats for {batch_id}\")\n",
    "    logging.info(f\"Average training loss (Y_pos): {avg_train_loss_1:.2f}\")\n",
    "    # logging.info(f\"Average training loss (Y_neg): {avg_train_loss_2:.2f}\")\n",
    "    logging.info(f\"Average evaluation loss (Y_pos): {avg_eval_loss_1:.2f}\")\n",
    "    # logging.info(f\"Average evaluation loss (Y_neg): {avg_eval_loss_2:.2f}\")\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        torch.save(model.state_dict(), f\"model_saved.pt\")\n",
    "\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Y_pos): 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155/1923553546.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "total_eval_loss_1 = 0\n",
    "# total_eval_loss_2 = 0\n",
    "test_batch_num = 0\n",
    "\n",
    "true_labels1 = []\n",
    "pred_labels1 = []\n",
    "# true_labels2 = []\n",
    "# pred_labels2 = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    test_batch_num += 1\n",
    "    if test_batch_num > 1:\n",
    "        break\n",
    "\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels1 = batch['labels1'].to(device)\n",
    "    # labels2 = batch['labels2'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits1 = model.classifier1(outputs.last_hidden_state[:, 0, :])\n",
    "        \n",
    "        # loss1 = loss_function(logits1, labels1)\n",
    "        \n",
    "        label = torch.argmax(labels1, dim=1)\n",
    "        # prediction = torch.argmax(logits1)\n",
    "        \n",
    "\n",
    "        true_labels1.extend(label.cpu().numpy())\n",
    "        pred_labels1.extend(torch.argmax(logits1, dim=1).cpu().numpy())\n",
    "\n",
    "    # total_eval_loss_1 += loss1.item()\n",
    "\n",
    "    del input_ids, attention_mask, labels1, logits1\n",
    "\n",
    "avg_train_loss_1 = total_train_loss_1 / test_batch_num\n",
    "avg_eval_loss_1 = total_eval_loss_1 / test_batch_num\n",
    "\n",
    "accuracy1 = accuracy_score(true_labels1, pred_labels1)\n",
    "\n",
    "# print(f\"Average training loss (Y_pos): {avg_train_loss_1:.2f}\")\n",
    "# print(f\"Average evaluation loss (Y_pos): {avg_eval_loss_1:.2f}\")\n",
    "print(f\"Accuracy (Y_pos): {accuracy1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get cuda allocation info or sth\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Perform operations that use CUDA memory\n",
    "\n",
    "# Get all objects in memory\n",
    "objects = gc.get_objects()\n",
    "\n",
    "# Filter objects that are tensors or models and print their CUDA memory usage\n",
    "for obj in objects:\n",
    "    if isinstance(obj, torch.Tensor) or isinstance(obj, torch.nn.Module):\n",
    "        memory_usage = obj.numel() * obj.element_size()\n",
    "        if obj.is_cuda:\n",
    "            print(f\"{type(obj).__name__}: {memory_usage / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
