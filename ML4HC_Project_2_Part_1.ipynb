{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INSTALLING AND IMPORTING LIBRARIES**"
      ],
      "metadata": {
        "id": "FLkWhb6Pnmcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "id": "sxIn8Y7inYpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppFzSQHNnKLC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import re\n",
        "\n",
        "import emoji\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "import random\n",
        "# from sklearn import metrics, model_selection, preprocessing\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "6DV9xsHEnim9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOADING THE DATASET**"
      ],
      "metadata": {
        "id": "A0GoCtILn4Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"bonus_cov19_sa.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "FeV_Zp7-nk6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"ID\", \"Unnamed: 0\"], axis = 1)"
      ],
      "metadata": {
        "id": "4a0b6thrn79y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA TO BE DONE**"
      ],
      "metadata": {
        "id": "s2C0xqR3nrEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA PRE-PROCESSING**"
      ],
      "metadata": {
        "id": "Doz7m9OfnxXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to correct spelling mistakes in a string\n",
        "def correct_spelling(text):\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "    for word in words:\n",
        "        corrected_word = spell.correction(word)\n",
        "        if corrected_word != None:\n",
        "          corrected_words.append(corrected_word)\n",
        "        # print(corrected_word)\n",
        "    corrected_text = ' '.join(corrected_words)\n",
        "    return corrected_text"
      ],
      "metadata": {
        "id": "1nRd0SzRoE2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abbreviations = {\n",
        "    \"w/\": \"with\",\n",
        "    \"w/o\": \"without\",\n",
        "    \"msg\": \"message\",\n",
        "    \"u\": \"you\",\n",
        "    \"r\": \"are\",\n",
        "    \"lol\": \"laughing out loud\",\n",
        "    \"np\": \"no problem\",\n",
        "    \"LOL\": \"laughing out loud\",\n",
        "    \"XD\" : \"laugh\",\n",
        "    \"xd\" : \"laugh\"\n",
        "}\n",
        "\n",
        "# Define a function to correct abbreviations in a string\n",
        "def correct_abbreviations(text, abbreviations):\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "    for word in words:\n",
        "        if word in abbreviations:\n",
        "            corrected_word = abbreviations[word]\n",
        "            corrected_words.append(corrected_word)\n",
        "        else:\n",
        "            corrected_words.append(word)\n",
        "    corrected_text = ' '.join(corrected_words)\n",
        "    return corrected_text"
      ],
      "metadata": {
        "id": "lZSR_72KoHd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweets(tweet):\n",
        "  # Convert all words to lower case\n",
        "  tweet = tweet.lower()\n",
        "\n",
        "  # Remove all punctuations\n",
        "  tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # Remove all stop words\n",
        "  # Tokenize the sentence\n",
        "  tokens = nltk.word_tokenize(tweet)\n",
        "  # Remove stop words\n",
        "  clean_tokens = [token for token in tokens if not token.lower() in stop_words]\n",
        "  # Join the tokens back into a sentence\n",
        "  tweet = ' '.join(clean_tokens)\n",
        "\n",
        "  # Removing HTML Tags, URLs and Website Links\n",
        "  # Removing Website Links\n",
        "  temp_text = tweet.split()\n",
        "\n",
        "  for i in temp_text:\n",
        "    if i[-4:] == \".com\" or i[:4] == \"www.\":\n",
        "      temp_text.remove(i)\n",
        "\n",
        "  tweet = ' '.join(temp_text)\n",
        "\n",
        "  # Remove HTML tags\n",
        "  clean_text = re.sub('<[^<]+?>', '', tweet)\n",
        "\n",
        "  # Remove URLs\n",
        "  clean_text = re.sub(r'http\\S+', '', clean_text)\n",
        "\n",
        "  tweet = clean_text\n",
        "\n",
        "  # Remove numbers\n",
        "  tweet = re.sub(r'\\d+', '', tweet)\n",
        "\n",
        "  # Deemojize Emojis\n",
        "  tweet = emoji.demojize(tweet)\n",
        "\n",
        "  # Correct Abbreviations\n",
        "  tweet = correct_abbreviations(tweet, abbreviations)\n",
        "\n",
        "  # Remove mentions\n",
        "  tweet = re.sub(r'@\\w+', '', tweet)\n",
        "\n",
        "  # Stemming\n",
        "  # Tokenize the sentence\n",
        "  tokens = nltk.word_tokenize(tweet)\n",
        "\n",
        "  # Stem the tokens\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "  # Join the stemmed tokens back into a sentence\n",
        "  tweet = ' '.join(stemmed_tokens)\n",
        "\n",
        "    # Spellcheck\n",
        "  try:\n",
        "    tweet = correct_spelling(tweet)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "  return tweet  "
      ],
      "metadata": {
        "id": "cq6RBUb0oJOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Tweet\"] = df[\"Tweet\"].map(preprocess_tweets)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zInumaW8oK-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the .csv of the preprocessed Dataframe\n",
        "\n",
        "df.to_csv(\"bonus_cov19_sa_preprocessed.csv\",index = False)"
      ],
      "metadata": {
        "id": "tNbk0_ptomee"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}